\documentclass[11point]{article}
% set the margins
\usepackage[left=25mm,right=25mm,top=20mm,bottom=20mm]{geometry}
% for displaying images
\usepackage{graphicx}
% for equation support, e.g. eqref
\usepackage{amsmath}
% for table support, e.g. toprule
\usepackage{booktabs}
% for multiple authors as a block
\usepackage{authblk}
% Custom enumerate item markers
\usepackage[shortlabels]{enumitem}
% for tables position
\usepackage{float}
% For in-text links
\usepackage{hyperref}
% For figure captions
\usepackage[font={small,it,sf},hypcap=true]{caption}
% for plots
\usepackage{pgfplots}
\usepackage{changepage}
\usepackage{tikz}
\usepackage{pgf}
\usetikzlibrary{graphs, shapes}

\newcommand{\note}[1]{\textbf{#1}}

\begin{document}

\title{INST0060 Investigation report:\\Hospital Queuing problem}
\author[*]{Group T1}
%\author{Auguste Baum}
%\affil[*]{Dept. of Information Studies}
%\affil[**]{Dept. of Physics \& Astronomy}
%\affil{Dept. of Natural Sciences}
\affil[ ]{University College London, WC1E 6BT}
%\affil[ ]{\textit {\{email1,email2\}@ucl.ac.uk}}
\date{\today}

\twocolumn[
\maketitle
]

\begin{abstract}
Queuing systems are a common subject of study in reinforcement learning. Here, we simulate a hospital with doctors and patients, each with their own characteristics, and use reinforcement learning to optimize the allocation of arriving patients to a queue.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\subsection{Background}
In recent times, hospital queuing times have become a concern.
Because hospital represent clusters of vulnerable individuals, it can be of vital interest to limit the number of people standing in line or in a waiting room.

According to a 2015 study, on average Americans spent 84 minutes per visit in the hospital during the 2003-2010 period. \cite{Ray2015}
%In hospital situat are 61\% people spend 90-180 minutes in the queue of clinic before being seen by doctor, whereas 36.1\% patients spent less than 5 minutes with the doctor in the consulting room.\cite{Oche2013} Unorganized queues in hospital are always a depressing experience for us. Differing from standing in line for coffee, queuing in hospital makes people stressful and their health is at risk.

A long-term goal to accommodate the increasingly large flow of patients would be to improve healthcare infrastructures.
Yet, because staffing and finance issues are so context-sensitive in many countries, it is not possible to directly ascertain which strategy to follow.
However, a more efficient allocation of pre-existing resources can be beneficial in many cases.
%Here are some good advice to manage the queues: using internal labelling to simplify the triaging process, arranging emergency cases over anything else and making some records to further improve patient information management.\cite{Kirill2017}
Furthermore, some decisions in such complex systems could potentially be automated for higher reactivity and better performance.

Reinforcement learning is a form of goal-directed learning with interaction;
an agent learns how to respond to situations with certain actions so as to maximize a reward.
\cite{sutton2018reinforcement}
In a sense, a reinforcement learning agent explores and exploits past experience.

Queuing problems have been optimised using reinforcement learning in the past (such as traffic intersections \cite{soh2007modelling}), and hospitals have been modelled statistically as well. \cite{feng2018steady,Hagen2013}
%For instance, Markov decision control was applied on a simulated traffic intersection (modelled as an M/M/1 queue) to minimize queue length and waiting time.

%Feng and Shi studied hospital inpatient flow management as a discrete-time queuing system.
%They identified an approximation to the steady-state distribution of the number of jobs in the system, and proved that their approximation performs better than those based on constant-time coefficients when the number of servers is small in a single hospital.
%Their research provide us good suggestions on how to describe the patients flow at hospital as discrete‚Äêtime process. \cite{feng2018steady}

%There is another research explored some different queue models in intensive care units (ICU) and its influence on waiting time to severe case patients. 
%This report built a system-based simulation model to analyse the patient flow before entering ICU.
%This report divided all the patients into 9 different classes that are categorized by severity of patients and length of waiting time in queue.
%Similarly, our research classified doctors of different types related to their abilities and patients of their priority based on their severity. \cite{Hagen2013}



In this work, we use reinforcement learning to allocate patients to queues as efficiently as possible.
First, we will describe the system as modelled (with simplifying assumptions) and the methods used to learn policies.
Then, we will analyse the resulting policies in simple cases, to gauge the ability of learners compared to systematic or random policies and to human intuition.
We will also reflect on our experience of the project, discussing the various difficulties encountered and what steps were taken in response.

%\section{Background}
\section{Methods}

In the first instance, we tried modelling the hospital system as a Markov decision process (or MDP) in continuous time, in order to maintain proximity with both a real hospital system and the material we took away from the INST0060 lectures.
However, we quickly realised that we were not equipped to simulate a system in continuous time,
and that the complexity of the system (even with our simplifying assumptions) prevented us from using an MDP or table-lookup approach.
Hence, we decided to approximate a hospital system by a discrete-time process with a patient arrival every timestep and developed featurisations that allowed for the use of function approximation.

\subsection{Model characteristics}
The hospital contains servers (doctors) of different types, which correspond to their abilities.
Similarly, jobs (patients) have different priorities that warrant different services.
For example, a doctor of type 1 can treat a patient with priority 1 or lower, but a doctor of type 0 \emph{cannot} treat a patient with priority 1 or higher.

In general, if the highest doctor type is $N$ at initialisation, then it is expected that there be at least one doctor of each type $n\in \{0,1,\ldots,N\}$.
Each \emph{type of} doctor has an associated queue, hence thereafter we call ``queue $n$'' the unique queue which doctors of type $n$ draw from.
Newly arrived patients have needs sampled from $\{0,1,\ldots,N\}$ according to a known distribution and are allocated to a given queue in $\mathcal{A}=\{0,1,\ldots,N\}$ by an agent following a policy $\pi$.
Note that, once allocated, a patient may not change queues, though they \emph{may be allocated to a doctor who cannot treat them} (see \autoref{fig:hospital_diagram}).
Once they are received by a doctor, if there is a discrepancy, the patient is sent away.

The state space is considered discrete, for simplicity. At each time step:
\begin{itemize}[nosep]
    \item a new patient arrives, with priority level sampled at random; 
    \item one or more doctors might be finished treating a patient; when that is the case, the ``cured'' patient leaves and is replaced by the next-in-line. 
\end{itemize}

Each individual doctor has their own efficiency, which is modelled by the probability of being done with a patient at each timestep.

Hence, if a skilled doctor happens to also be very fast, it might be beneficial to allocate low priority patients to their queue, to deplete the size of queues more quickly;
however, this runs the risk of making patients with high priority wait longer.

In this study, we restrict ourselves one doctor per type, as this still offers ample flexibility. The general case could be the subject of future work.
Similarly, though this study started with one queue \emph{per doctor} rather than per \emph{type} of doctor, we simplified the system after discussions with the module administrators and TAs.

\begin{figure}
  \centering
    \begin{adjustwidth}{-0.2cm}{}
    \resizebox{\columnwidth}{!}{%
        \input{figures/hospital.tex}
    }
    \end{adjustwidth}
    \caption{A hospital with 4 doctors: two of type 0 ($D_0$ and $D_1$, green), one of type 1 ($D_2$, orange) and one of type 2 ($D_3$, red). A newly arrived patient with priority 1 is being allocated to a queue according to policy $\pi$.
    Note the priority 2 patient mistakenly sent to queue 0; they will not be sent away until they have reached the end of the queue.}
    \label{fig:hospital_diagram}
\end{figure}

One of the challenges associated with learning a policy to set balanced priorities between curing patients and keeping the hospital from getting too full.
Because of the behaviour of misallocated patients (see \autoref{fig:hospital_diagram}), it can be more efficient for the agent to just misallocate patients from the start so as to keep the hospital empty.

The agent's actions tend to have very delayed consequences, which makes it difficult for it to link rewards and policy.
Further, originally the penalty for misallocation was set to trigger only when patients reached the doctor.
This mechanic was modified midway through the project to investigate its effect, and this will be the subject of Experiment 3.

\subsection{Learning}
Two well-known algorithms of RL are the $Q$-learning (QL) and the SARSA algorithms. \cite{sutton2018reinforcement}
SARSA is an on-policy algorithm,
meaning the agent learns the value of the state-action pair on the basis of the performed action.
By contrast, QL is an off-policy algorithm, where the state-action-value function (or \textit{q}-function) is estimated based on a greedy policy, which is not necessarily the currently implemented policy.
Generally, SARSA tends to exhibit faster convergence, while the performance of $Q$-learned policies tend to be higher. 
However, in SARSA the learner can easily get stuck in local minima, and QL tends to take longer to converge.

\subsection{Featurisation}

Even with the limited complexity of its modelling, the number of individual states in the hospital is extremely large, since each state includes the number of people in each queue, but also the types of patients and their waiting times.
In fact, it is countably infinite if the hospital occupancy in not bounded (which it is during training), so that a simpler representation (a simple vector) must be used for the agent to learn efficiently.

While at the beginning a few real numbers were used (such as the average number of patients per queue), later on an effort was made to include more information (real numbers, but converted to discrete values) and finally, one-hot vectors.
This last attempt was the most successful and will be applied throughout this work; the impact of featurisations will be the subject of experiment 1.

In this work, the systems of interest will be:
\setlist{left=0pt}
% Could use description environment
% I want to text to wrap all the way back because it's a waste of space
% Considering just breaking line after "Experiment:"
\begin{enumerate}[\bfseries {Experiment} 1:, wide, labelwidth=!, labelindent=0pt]
    %\DrawEnumitemLabel
    \item Two doctors (one of type 0 and one of type 1) with one featurisation and one learning algorithm. The 
    \item
    We analyse the behaviour of the agent as the frequency of urgent patients increases.
\end{enumerate}{}

%Featurisation in machine learning is the procedure of processing data to create features that makes machine learning algorithms work. If featurisation is built correctly, it increases the accuracy of prediction in machine learning algorithms by raw data processing. 

%In the simple example of Markov Decision Process of Stair Climbing, there are only two actions: climbing upstairs and going downstairs. Value function assigns values to states, and after definite updates, the value function in this model will remain unchanged values. The matrix of states in the stair climbing example also have fixed rows and columns, in which columns are the number of stairs and rows represents the values functions. 

%However, in our model of queueing problem in hospital, the states can be consider as infinite number. The patients are assigned to different queues, which correspond to their severity of illness. Similarly, the doctors have different types based on their abilities. Moreover, in each queue there could be a different amount of patients with different severities and wait times. To capture all the information, we could set the models as a limit of 30 people per queue, wait time of 40 time steps and 6 types of doctors totally. In this case, we could have more than $(30\times40\times3)^3 \approx 4\cdot10^{10}$ states. It is impossible to learn infinite number of states in our model. As a result, featurisation makes a great difference here to simplify our model with limited states. 
%Here are the basic steps of applying featurisation.
%\begin{enumerate}
%    \item Brainstorm on the classification of data and features.
%    \item Create features and set range of classification. 
%    \item Check the efficiency of feature the how good it works on model.
%    \item Start again from beginning until the features work perfectly.
%\end{enumerate}

%To discover a good final policy, six featurisations of our hospital model have been designed as follows.

%All featurisations include the average number of patients waiting in the different queues. Both Featurisation 1 and 2 use different thresholds to compare whether a given queue has more or fewer patients with a certain need. F1 compares the number of needy patients with thresholds 1 and 3 respectively and results in four conditions: no patients, one patient, patient number between 1 and 3, patients more than 3 and corresponsively update the list by 0, 1, 2, and 3. F2 updates the list every time when the patient number is not smaller than 2. 

%For F3, other than the first element, the following elements in the list represent the number of different patients with needs in each queue only and it adds total number of different patients in separate queues. F4 includes an additional element of the waiting time based on F3. For F5, the first element in the list is the need of the newly arrived patient, followed by the total number of patients in all the queues, the average need and waiting time in each queue.



%\subsection{Learning Algorithm}
%For the learning agent in Reinforcement Learning algorithm, there are two types of policy:
%\begin{itemize}
%    \item On-Policy: The learning agent learns the value function according to the current action derived from the policy currently.\cite{sutton2018reinforcement}
%    \item Off-Policy: The learning agent learns the value function according to the action derived from another policy.\cite{sutton2018reinforcement}
%\end{itemize}
%Sarsa (State-Action-Reward-State-Action) algorithm is an on-policy algorithm, which follows the policy it is evaluating. It uses the action performed by the current policy to learn the Q-value.
%$Q$-learning algorithm is slightly different from SARSA. $Q$-learning technique is an off-policy technique and uses the greedy approach to learning the Q-value.
%
%\subsubsection{Sarsa}
%Equation of Sarsa can be described as:
%\[
%\hat{q}(s_t,a_t)
%\leftarrow
%\hat{q}(s_t,a_t) +
%\alpha\left[
%    r_{t+1} +
%    \gamma\hat{q}(s_{t+1},a_{t+1}) -
%    \hat{q}(s_t,a_t)
%\right]
%\]
%As with Monte-Carlo, Sarsa learning can perform on-policy optimisation with temporal difference estimates using Q-estimates. The equations present the current state($s_t$), current action($a_t$), reward obtained ($r_{t+1}$), next state ($s_{t+1}$) and next action ($a_{t+1}$). This observation ($s_t$, $a_t$, $r_{t+1}$, $s_{t+1}$,  $a_{t+1}$) stands for the name of Sarsa: State Action Reward State Action. In our example of queue problem in hospital, current state represents the status of the patients in the queue and actions for patients are their choices to joining which doctor's queue according on their severity of illness.\\
%Our process of codes can be described as following seven steps:\\
%1. Import the required libraries and build the environment\\
%2. Define utility function (Sarsa) in learning process\\
%3. Initialize parameters of this function\\
%4. Training the learning agent\\
%5. Application of policies\\
%6. Evaluate the performance\\
%7. Visualisation of result
%\subsubsection{Q-Learning}
%$Q$-learning is an off-policy algorithm for TD learning. It learns the optimal policy even when actions are selected based on a more exploratory or even random policy. Its equation can be introduced as:
%
%\[\hat{q}(s_t,a_t)\leftarrow \hat{q}(s_t,a_t) + \alpha\left[r_{t+1}+\gamma \max_{a}\hat{q}(s_{t+1},a)-\hat{q}(s_t,a_t)\right] \]
%
%In which, $s$(state) and $a$(action) has the same definition as Sarsa's equation. $\alpha$  presents the the step length taken to update the estimation of q value. $\gamma$ is a discount factor for future rewards with range from $0$ to $1$. $r_{t+1}$ is the reward observed response to current action.



\section{Results}

\subsection{Experiment 1: Featurisations (code in \texttt{feat\_exp.py})}

In this experiment, we compare the performance of different featurisations. The hospital has six doctors, of types 0 through 5. Here, doctors 0, 1 and 4 are inefficient (0.1), doctors 3 and 5 are efficient (0.4 and 0.5, respectively) and doctor 2 is highly efficient (0.9). The system is deliberately complex, so that patient allocation is not trivial, but still intuitive in a way for an observer. Patients of different types arrive with equal probability. Here we use the SARSA algorithm, but this is not a significant choice since our main concern are the featurisations. Furthermore, the learning process consists of 100 episodes with 100 steps each. The learning process is repeated 100 times for each feature so that we can get consistent data. This is essential since our model incorporates a certain level of randomness.

The most important characteristic of the current model, apart from the definition of the used featurisation, is the reward system. In its current state, the system gives a penalty as soon as a misallocation happens, another proportional to the waiting time of patients and also incorporates a constant positive reward for each treated patient. (******* in discussion at the end  - The constant positive reward for a treated patient mitigates misallocation of patients as well. Before it was introduced the system was occasionally willing to assign patient to wrong (but empty)
queues and in that way get rid of them faster and with small penalty.)

We have created a variety of featurisations and they can all be found in \texttt{learning.py} file. They generally fall into two categories: one-hot vectors (featurisations 7-12) and non-one-hot vectors (featurisations 1-6). We will now describe further the construction of some of them and their efficiency.

\texttt{Feature 1} is an iteration of the first feature we created. The information it holds is, essentially, the type(a single number) of the next patient that has to be allocated and indicators about how many people of each type are in any given queue. Using this feature the algorithm does exhibit some learning behaviour but it is far from being satisfactory or consistent. Namely, the system seems to recognize which doctors are more efficient but fails to properly allocate the patients. This issue arises due to a combination of factors. First, the new patient's type is represented by a single number and this potentially hinders the algorithm from properly distinguishing between the different patients. Furthermore, the feature vector conveys proper information about the distribution of patients in different queues only when they are approximately equally distributed and in small numbers (up to 2 persons of a particular type in a queue). On top of that, feature 1 does not hold any information about the waiting time of patients.

\texttt{Feature 7} is a one-hot vector, which represents the information from \texttt{feature 1} in a better way and also includes the mean waiting time of the waiting patients. All the needed characteristics of the system are now encoded as one-hot vectors which are concatenated. For example, if we have 4 doctors (of different types) and the new patient is of type 0 then their representation would be [1, 0, 0, 0]. The distribution of patients and the waiting are constructed in a similar way(i.e. to each characteristic a number is assigned, which is then converted into a one-hot vector). The system now produces results that are evidently better than these given by the simple naive policy. We also note that this featurisation consistently allocates patients to doctors who can actually treat them. 

\texttt{Feature 12} is essentially a slightly upgraded version of \texttt{feature 7}. Instead of holding information about mean waiting time, this feature takes into consideration the waiting time of the first patient from each queue. This allows to make an even better distinction between the queues. As can be seen below, this change improves the mean/median results marginally but most importantly it reduces the standard deviation of the total reward by 50\%.

In conclusion -> Why are one-hot vectors better, features should be built around the reward system\\*


\pgfplotsset{width=8cm,compat=1.8}
\begin{figure}
    \centering

\begin{adjustwidth*}{-0.2cm}{}
    \input{figures/figure.tex}
    \caption{Caption}
    \label{fig:my_label}

\end{adjustwidth*}
\end{figure}

Will add info about more features.

Will add a plot for the reward evolution of all the features perhaps

Also some plot about allocation/misallocation of patients


\subsection{Experiment 2: Model behaviour (code in \texttt{fastdoc\_exp.py})}
In this experiment, we try to characterise the behaviour of the agent in ambiguous situations.
The hospital has four doctors, of types 0 through 3. Doctor 3 (the most highly skilled) is very efficient (80\% chance of treating a patient at each time step), while lower skilled doctors are equally slow (40\%).

We set the featurisation to \texttt{feature\_12}, the learning algorithm to SARSA and the reward system to penalise misallocations early and not penalise when occupancy is reached, as these seem to yield better performance overall. These assumptions are the object of experiments 1 and 3.
New patients are equally likely to be of type 0, 1 or 2. 
We vary the probability of type 3 patient arrivals during training, from 10\% to 90\%.
When there are few urgent patients, we would expect the agent to allocate many low priority patients to doctor 3 because it treats more quickly. However, when there are many, allocating low-priority patients to doctor 3 causes type 3 patients to wait longer, which is detrimental to the reward.

For each data point, we train an agent on a hospital with a given arrival probability and then apply the learned policy to a hospital with \emph{equiprobable} arrivals so that policies can be compared on equal footing. 
We record the proportion of patients of type 3 allocated to queue 3, the \emph{overall} proportion of patients allocated to queue 3, and the average time waited by a patient that has seen a doctor.

Throughout the experiment, the misallocation rate for type 3 patients was close to 0 (i.e. type 3 patients were almost systematically put in queue 3), so we will be focusing on the other quantities with the knowledge that the different policies can be reliably compared.

We start by running a ``short-term'' trial, in which for each probability, the simulation (training and testing) was carried out 300 times; for each simulation the agent was trained for 50 episodes with 500 steps each.The results are shown in \autoref{fig:fast_doc}.

We mentioned earlier that we should compare policies on an equal footing. However, this is not necessarily possible considering the agent might not have had equal access to each state in each circumstance: when type 3 patients arrive 90\% of the time, the agent gets little experience with treating lower priority patients.
In this case, since patients of type 3 can only be put in queue 3 (any other decision would result in a misallocation penalty), the agent learns to put any arriving patient in queue 3, irrespective of their type: this is reflected in \autoref{fig:fast_doc}.

Yet, the behaviour for low probability agrees with our predictions.
\begin{figure}[H]
    \centering
    % \includegraphics{figures/???}
    \resizebox{\columnwidth}{!}{
        \input{figures/exp2_queues.tex}
    }
    \caption{Median proportion of patients allocated to queue 3 as priorities change. Error bars show $20^\text{th}$ and $80^\text{th}$ percentile (to represent the high skew).}
    \label{fig:fast_doc}
\end{figure}

In response to this extraneous, though predictable behaviour, we also ran a ``long-term'' trial, with the same parameters and training lasting 150 episodes instead of 50.
This way, the model can gather more meaningful experience with each type of patient during training, even when arrivals of some patient type are sparse.
For each point, the simulation was carried out 100 times (which took around 4 processor hours on a modern laptop).
The result is in \autoref{fig:fast_doc}.

It would seem that in general the agent is more cautious about allocating patients to queue 3, particularly when urgent patients are frequent (the probability is about 25\%, which corresponds to only the type 3 patients being put in queue 3).

Accordingly, the waiting time of type 3 patients is drastically decreased when the type 3 queue is reserved for them (red bar on the right of \autoref{fig:fast_doc}): the median waiting time in this case is 0.1 timesteps whereas it sits between 30 and 50 timesteps when all patients are allocated to queue 3 (blue bar on the right of \autoref{fig:fast_doc}).

\subsection{Experiment 3: Penalty check (code in \texttt{rewards\_exp.py})}
Experiment 3 tests and compares the efficiency of queues with different combinations of two penalties, i.e. the penalty for misallocation (\texttt{earlyRewards}) and the penalty for reaching the occupancy of the hospital (\texttt{capacity\_penalty}). When the former one is true, the penalties, proportional to time waited, are issued directly and immediately when a patient is sent to a specific queue incorrectly before the patient reaches the doctor. When the later one is true, it means that a penalty for inefficiency, which is extremely large, should be taken into account when the maximum capacity of the hospital is reached either at or before the episode terminates.

Figure 4 depicts the distribution of the frequency rate of misallocation with capacity = 100, steps =100, episodes =10. It can be thus inferred that SARSA contains a higher rate of misallocation than Q-Learning does, indicating that Q-Learning might have a better performance in improving the efficiency of queues. The standard deviations of SARSA are relatively large, which also shows that the output values of SARSA are spread out over a wider range.

This experiment also compares the results when we apply two different rewards. Firstly, we create early reward in our model. When it is true, the rewards are allocated directly when the patient is sent to a specific queue, which represents that the patients in hospital are allocated to each queue timely. Its rewards are calculated proportionally according to the waiting time of patients.
Besides, there is a penalty for full capacity of hospital. Capacity penalty equals True means when the capacity is reached not only the episode terminates but also the  maximum capacity of hospital. In \texttt{hospital.py} file, we create a function to compare the length of all the queues to the occupancy of hospital. Large number of negative reward is accumulated if the former is bigger. 
In experiment 3, early rewards and capacity penalty are divided into four categories. As showed in Figure 3, TT, TF, FT and FF stand for true or false of early rewards and capacity penalty respectively. For example, TF means early rewards equals true and capacity penalty equals false.

\begin{figure}
    \centering
    \input{figures/misalloc_dist.tex}
    \caption{The frequency rate of misallocation}
    \label{fig:misalloc_dist}
\end{figure}

Figure 3 summarizes the performance of Sarsa and Q learning with early reward and capacity penalty. X-axis can be explained as their status of applying penalties and y-axis is the occurrence frequency of misallocation of patients in hospital. Black line in the middle of each bar represents standard deviation of input values. Overall, Sarsa algorithm has higher probability to misallocate patients in the wrong queues than Q learning. In addition, large frequency appears when we only apply capacity penalty or not request both early reward and capacity penalty.

\begin{figure}
    \centering
    \input{figures/exp3_misalloc.tex}
    \caption{The frequency rate of misallocation}
    \label{fig:penalty_diagram}
\end{figure}


\section{Discussion}
Our experiments shed some light on various aspects of our approach to modelling and controlling the hospital queuing system.
First, using the internal state directly to learn a policy, even for our simplified model, is intractable. This lead us to experiment with different representations for the hospital.

It would seem that featurisations in the form of a one-hot vector are superior in terms of the performance of the resulting policy.
This was the subject of experiment 1, in which we could also see that learned policies with the right featurisation outperforms the na√Øve policy.

In experiment 2, we compared the behaviour of the learned agent to our intuition in a simple example, where one doctor is both very experienced and very efficient.
Though the measures are affected by the training processes itself, it seems that with enough training the agent adopts coherent behaviour with respect to our reward system (which is based on our subjective intuition, admittedly).


\note{We did not look at how learning parameters affected learning (e.g. learning rate or discount parameter). We just stuck to what we thought were sensible choices.}
\section{Conclusion}


\section*{Declaration}
This document represents the group report submission from the named authors for the project assignment of module: Foundations of Machine Learning and Data Science (INST0060), 2019-20. In submitting this document, the authors certify that all submissions for this project are a fair representation of their own work and satisfy the UCL regulations on  plagiarism.

% to put references in
\bibliography{report}
% define the bibliography style
\bibliographystyle{plain}

\end{document}
